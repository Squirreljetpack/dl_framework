# 1

See `cellv.ipynb`



## 1

Mean error about 20 for regression,
acc about 70% for classifier

See notebook

## 2-4

Dropout is where a certain number of 

Best for classifier:
activation:
momentum:
learning rate:
dropout:



## 5

No improvement since classifier just predicts one class

# 2

see `pinn.ipynb`

## 1

See notebook

## 2

I used the same MLP architecture class as above with GELU.

This means an input (Linear+activation) followed by a certain number of blocks (Linear(hidden_size) + activation + dropout), followed by a FC output. The input and output dimensions are both batch_size x 1.

I tried 0 and 1 blocks because I figured simple behavior like SHO didn't have too much complexity and that a single layer was enough to fit/overfit, but more may improve training (hence trying 1 block).



## 3

It was hard to tune the model because physics loss would not propogate.
I got errors like `One of the differentiated Tensors does not require grad` or `element 0 of tensors does not require grad and does not have a grad_fn`, or autograd.grad producing None values. Somehow calling `model(input)` was not building the correct backprop graph it seemed. Placing the whole thing within a `with torch.set_grad_enabled(True):` finally enabled everything to go through but it definitely feels like something larger is wrong.

I tried various learning rates, and used a large batch size, opting to choose lower lr to compensate, so as to speed up training. I also experimented with a lr_scheduler, one_cycle_LR and noticed some different loss behavior (several spikes), and decided to keep it since slowing the learning rate seemed desirable.

I also configured a scalar value for the physics loss, so that loss was computed as `data_loss + alpha * physics_loss`, and found a small value like `0.1` worked well.



## 4

Within the domain, the model performed very well and adhered tightly.

However, I had a lot of trouble getting physics loss to compute correctly so the model didn't actually learn the correct parameter values.

## 5, 6

Done and done.
Again, because of physics loss trouble, the model didn't learn to extrapolate.



# 3

## 